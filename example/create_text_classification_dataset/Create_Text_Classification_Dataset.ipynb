{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this tutorial, we are going to build a text dataset for category classification. After this tutorial,\n",
    "we will have a basic picture of how to use DataCI to manage different versions of datasets,\n",
    "their data generating pipelines, and quickly adapt previous data scientists efforts to new versions of datasets.\n",
    "\n",
    "This tutorial uses a simplified workflow from industry. Given a product title, we are going to determine the product\n",
    "category.\n",
    "\n",
    "# 0. Prerequisites\n",
    "\n",
    "## Initialize DataCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.cloud.aliyuncs.com/pypi/simple/\n",
      "Requirement already satisfied: scikit-learn in /root/miniconda3/envs/dataci/lib/python3.8/site-packages (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /root/miniconda3/envs/dataci/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /root/miniconda3/envs/dataci/lib/python3.8/site-packages (from scikit-learn) (1.21.6)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /root/miniconda3/envs/dataci/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /root/miniconda3/envs/dataci/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: http://mirrors.cloud.aliyuncs.com/pypi/simple/\n",
      "Requirement already satisfied: transformers in /root/miniconda3/envs/dataci/lib/python3.8/site-packages (4.27.2)\n",
      "Requirement already satisfied: requests in /root/miniconda3/envs/dataci/lib/python3.8/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/miniconda3/envs/dataci/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/envs/dataci/lib/python3.8/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/dataci/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /root/miniconda3/envs/dataci/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/envs/dataci/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/envs/dataci/lib/python3.8/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /root/miniconda3/envs/dataci/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/dataci/lib/python3.8/site-packages (from transformers) (3.9.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/dataci/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/dataci/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/dataci/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/miniconda3/envs/dataci/lib/python3.8/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/dataci/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "You should also install pytorch, check https://pytorch.org/get-started/locally/ to find specific version matches your OS, package and platform\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n",
    "%pip install transformers\n",
    "print(\n",
    "    'You should also install pytorch, check https://pytorch.org/get-started/locally/ to find specific version '\n",
    "    'matches your OS, package and platform'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/workspace/DataCI\n"
     ]
    }
   ],
   "source": [
    "%cd ../../\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['PYTHONPATH'] = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized DVC repository.\n",
      "\n",
      "\u001b[31m+---------------------------------------------------------------------+\n",
      "\u001b[0m\u001b[31m|\u001b[0m                                                                     \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m        DVC has enabled anonymous aggregate usage analytics.         \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m     Read the analytics documentation (and how to opt-out) here:     \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m             <\u001b[36mhttps://dvc.org/doc/user-guide/analytics\u001b[39m>              \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m                                                                     \u001b[31m|\u001b[0m\n",
      "\u001b[31m+---------------------------------------------------------------------+\n",
      "\u001b[0m\n",
      "\u001b[33mWhat's next?\u001b[39m\n",
      "\u001b[33m------------\u001b[39m\n",
      "- Check out the documentation: <\u001b[36mhttps://dvc.org/doc\u001b[39m>\n",
      "- Get help and share ideas: <\u001b[36mhttps://dvc.org/chat\u001b[39m>\n",
      "- Star us on GitHub: <\u001b[36mhttps://github.com/iterative/dvc\u001b[39m>\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python dataci/command/init.py -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Download Sample Raw Data\n",
    "\n",
    "Assume we have sampled 20K raw data from online product database, and hand over these raw data to annotators for\n",
    "verify their product category which are filled by sellers and contains some noise. Now, the first batch of\n",
    "10K finish labelling data are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# saved at data/text_raw/\n",
    "mkdir -p data\n",
    "rm -r data/*\n",
    "cp -r dataset/text_cls_v1 data/text_raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "This dataset contains train and val splits. Each split contains a CSV file with 3 columns:\n",
    "`id`, `product_name` and `category_lv3`. We are going to build a pipeline to classify the product category\n",
    "(`category_lv3`) based on its dirty `product_name`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Build Text Classification Dataset\n",
    "\n",
    "## 1.1 Publish raw data\n",
    "\n",
    "Add this dataset with two split into the data repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:dataci.dataset.publish:Caching dataset files: data/text_raw/train.csv\n",
      "\u001b[?25l                                                                          \u001b[32m⠋\u001b[0m Checking graph\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "  0% Checking cache in '/root/workspace/DataCI/.dvc/cache'| |0/? [00:00<?,    ?f\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Transferring                          0/? [00:00<?,     ?file/s]\u001b[A\n",
      "  0%|          |Transferring                          0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "100% Adding...|████████████████████████████████████████|1/1 [00:00, 58.77file/s]\u001b[A\n",
      "\u001b[0mINFO:dataci.dataset.publish:Adding dataset to db: text_raw_train@f3a821d8a22533c642f16f115d5d2b3f02569f0d\n",
      "INFO:dataci.dataset.publish:Caching dataset files: data/text_raw/val.csv\n",
      "\u001b[?25l                                                                          \u001b[32m⠋\u001b[0m Checking graph\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "  0% Checking cache in '/root/workspace/DataCI/.dvc/cache'| |0/? [00:00<?,    ?f\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Transferring                          0/? [00:00<?,     ?file/s]\u001b[A\n",
      "  0%|          |Transferring                          0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "100% Adding...|████████████████████████████████████████|1/1 [00:00, 80.02file/s]\u001b[A\n",
      "\u001b[0mINFO:dataci.dataset.publish:Adding dataset to db: text_raw_val@641a430201153db41a736961eea5299e53955fdc\n"
     ]
    }
   ],
   "source": [
    "!python dataci/command/dataset.py publish -n text_raw_train data/text_raw/train.csv\n",
    "!python dataci/command/dataset.py publish -n text_raw_val data/text_raw/val.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.2 Build a dataset for text classification\n",
    "\n",
    "1. Build train dataset v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dataci/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A       \n",
      "../../.dataci/tmp/text_raw_train/f3a821d8a22533c642f16f115d5d2b3f02569f0d/train.\n",
      "csv\n",
      "Modifying stage 'text_augmentation' in 'dvc.yaml'\n"
     ]
    }
   ],
   "source": [
    "import augly.text as txtaugs\n",
    "\n",
    "from dataci.pipeline import Pipeline, stage\n",
    "\n",
    "\n",
    "# Data processing: text augmentation\n",
    "@stage(inputs='text_raw_train', outputs='text_aug.csv')\n",
    "def text_augmentation(inputs):\n",
    "    transform = txtaugs.InsertPunctuationChars(\n",
    "        granularity=\"all\",\n",
    "        cadence=5.0,\n",
    "        vary_chars=True,\n",
    "    )\n",
    "    inputs['product_name'] = inputs['product_name'].map(transform)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Define data pipeline\n",
    "train_data_pipeline = Pipeline(name='train_data_pipeline', stages=[text_augmentation])\n",
    "train_data_pipeline.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Run the train data pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running stage 'text_augmentation':\n",
      "> python code/text_augmentation.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dataci.pipeline.stage:Load input /root/workspace/DataCI/.dataci/tmp/text_raw_train/f3a821d8a22533c642f16f115d5d2b3f02569f0d/train.csv as pandas Dataframe\n",
      "INFO:dataci.pipeline.stage:Save output to feat/text_aug.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating lock file 'dvc.lock'\n",
      "Updating lock file 'dvc.lock'\n",
      "Use `dvc push` to send your updates to remote storage.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dataci.run.run.Run at 0x7f2b544b8d60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The output `text_aug.csv` will be used as train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Run training with the built train and val dataset v1\n",
    "Now you can simple train a pre-trained BERT on this text classification dataset v1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id                                       product_name category_lv0\n",
      "0  2197727145  [\"Ziplo;ck Se:al Tr;anspa?rent -Stora'ge Ba:g ...         FMCG\n",
      "1  6946156365  [\"Lab O,n Hai.r Ant'i Hai?r Fal.l Sha'mpoo ;30...         FMCG\n",
      "2  3865988017  [\"กระปุ-กออมส!ิน AT!M กระ;ปุกออ;มสิน ,มีดนต?รี...         FMCG\n",
      "3  1585616576  [\"Siêu ?Sim D!ata 4:G Trọ?n Gói! 1 Nă!m Khô;ng...           EL\n",
      "4   733610874  [\"Quần 'Jean ...Nữ Ốn...g Loe: Lưng, Cao !Aaa ...      Fashion\n",
      "           id                                       product_name category_lv0\n",
      "0  3373941853    Adorn by Calmskin Blueberry Whipped Scrub 250ml         FMCG\n",
      "1  5948198349  Kacamata Hitam Korean Fashion Wanita/Pria Sung...      Fashion\n",
      "2    11608450  de Nature - Kapsul  Ziirzax dan Typhogell - Ob...         FMCG\n",
      "3  2107761836  Samsung Galaxy A73 5G | 8GB+128GB | 8GB+256GB ...           EL\n",
      "4  3251354065  NEW LABEL SKEENCARE PEELING LOTION TRIO 100ML ...         FMCG\n",
      "Train split size: 16204, Val split size: 853\n",
      "Test dataset size: 1896\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[Epoch0][Step 0] train_loss=1.24275803565979, train_acc=0.75\n",
      "[Epoch 0] train_loss_epoch=1.4089665087786587, train_acc_epoch=0.26136363636363635\n",
      "[Epoch0][Step 0] val_loss=1.307714581489563, val_acc=0.25\n",
      "[Epoch: 0] val_loss_epoch=1.411566517569802, val_acc_epoch=0.18181818181818182\n",
      "[Epoch1][Step 0] train_loss=1.553436279296875, train_acc=0.25\n",
      "[Epoch 1] train_loss_epoch=1.3869466619058088, train_acc_epoch=0.3181818181818182\n",
      "[Epoch1][Step 0] val_loss=1.296889066696167, val_acc=0.25\n",
      "[Epoch: 1] val_loss_epoch=1.4441949183290654, val_acc_epoch=0.19318181818181818\n",
      "[Epoch2][Step 0] train_loss=1.38037109375, train_acc=0.5\n",
      "[Epoch 2] train_loss_epoch=1.3884882439266553, train_acc_epoch=0.23863636363636365\n",
      "[Epoch2][Step 0] val_loss=1.2911078929901123, val_acc=0.25\n",
      "[Epoch: 2] val_loss_epoch=1.4364083951169795, val_acc_epoch=0.18181818181818182\n",
      "[Step 0] test_loss=1.5123263597488403, test_acc=0.25\n",
      "test_loss_epoch=1.439349510452964, val_test_epoch=0.20454545454545456\n"
     ]
    }
   ],
   "source": [
    "!python example/create_text_classification_dataset/train.py \\\n",
    "  --train_dataset=train_data_pipeline/latest/runs/1/feat/text_aug.csv \\\n",
    "  --test_dataset=data/text_raw/val.csv \\\n",
    "  -b4 \\\n",
    "  --max_train_steps_per_epoch=20 \\\n",
    "  --max_val_steps_per_epoch=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For demonstration purpose, we only train and validation the dataset for a few steps and obtain the results.\n",
    "\n",
    "3. Save data pipeline\n",
    "\n",
    "You can now publish your data pipeline for a better management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data_pipeline.publish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4. Publish first version of text dataset\n",
    "\n",
    "Run the published pipeline `train_data_pipeline`, its final output `text_aug.csv` will be\n",
    "automatically published as a dataset: `train_data_pipeline:text_aug`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dataci.run.save:Recover dvc file feat/text_aug.csv.dvc\n",
      "INFO:dataci.dataset.publish:Caching dataset files: feat/text_aug.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 'text_augmentation' is cached - skipping run, checking out outputs\n",
      "Generating lock file 'dvc.lock'\n",
      "Updating lock file 'dvc.lock'\n",
      "Use `dvc push` to send your updates to remote storage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: output 'feat/text_aug.csv' is already specified in stage: 'text_augmentation'.\n",
      "INFO:dataci.dataset.publish:Adding dataset to db: train_data_pipeline:text_aug@db07fae33d424a11445a96aafc182175abe255d7\n",
      "INFO:dataci.pipeline.pipeline:text_raw_train@f3a821d8a22533c642f16f115d5d2b3f02569f0d >>> train_data_pipeline@bcca51d.run1 >>> train_data_pipeline:text_aug@None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dataci.run.run.Run at 0x7f2b54492a00>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Try with New Data Augmentation Method\n",
    "\n",
    "Let's create a second version of `train_data_pipeline:text_aug` for text classification with\n",
    "different data augmentation method to improve the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.1 Write a second version train data pipeline\n",
    "\n",
    "We design a better data augmentation method for `train_data_pipeline_v2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@stage(inputs='text_raw_train', outputs='text_aug.csv')\n",
    "def text_augmentation(inputs):\n",
    "    transform = txtaugs.Compose(\n",
    "        [\n",
    "            txtaugs.InsertWhitespaceChars(p=0.5),\n",
    "            txtaugs.InsertPunctuationChars(\n",
    "                granularity=\"all\",\n",
    "                cadence=5.0,\n",
    "                vary_chars=True,\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    inputs['product_name'] = inputs['product_name'].map(transform)\n",
    "    return inputs\n",
    "\n",
    "train_data_pipeline_v2 = Pipeline(name='train_data_pipeline', stages=[text_augmentation])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.2 Test the pipeline v2 and publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 'text_augmentation' is cached - skipping run, checking out outputs\n",
      "Generating lock file 'dvc.lock'\n",
      "Updating lock file 'dvc.lock'\n",
      "Use `dvc push` to send your updates to remote storage.\n",
      "Modifying stage 'text_augmentation' in 'dvc.yaml'\n"
     ]
    }
   ],
   "source": [
    "train_data_pipeline_v2.build()\n",
    "train_data_pipeline_v2()\n",
    "train_data_pipeline_v2.publish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.3 Let's check our pipeline `train_data_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_pipeline\n",
      "|  Version\tCreate time\n",
      "|- bcca51d\t2023-03-24 13:51:00\n",
      "|    |- run1\n",
      "|- ceb5abe\t2023-03-24 13:51:15\n"
     ]
    }
   ],
   "source": [
    "!python dataci/command/pipeline.py ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.3 Publish text classification dataset v2\n",
    "\n",
    "It is easy to update output dataset once our data pipeline have new version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running stage 'text_augmentation':\n",
      "> python code/text_augmentation.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dataci.pipeline.stage:Load input /root/workspace/DataCI/.dataci/tmp/text_raw_train/f3a821d8a22533c642f16f115d5d2b3f02569f0d/train.csv as pandas Dataframe\n",
      "INFO:dataci.pipeline.stage:Save output to feat/text_aug.csv\n",
      "INFO:dataci.run.save:Recover dvc file feat/text_aug.csv.dvc\n",
      "INFO:dataci.dataset.publish:Caching dataset files: feat/text_aug.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating lock file 'dvc.lock'\n",
      "Updating lock file 'dvc.lock'\n",
      "Use `dvc push` to send your updates to remote storage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: output 'feat/text_aug.csv' is already specified in stage: 'text_augmentation'.\n",
      "INFO:dataci.dataset.publish:Adding dataset to db: train_data_pipeline:text_aug@c5a9a4941789584da67253aaf6eafc4ae5820977\n",
      "INFO:dataci.pipeline.pipeline:text_raw_train@f3a821d8a22533c642f16f115d5d2b3f02569f0d >>> train_data_pipeline@ceb5abe.run1 >>> train_data_pipeline:text_aug@None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dataci.run.run.Run at 0x7f2b544bc7f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pipeline_v2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Try with more raw data\n",
    "\n",
    "Our human annotators have finished the 2nd batch 10K data labelling. We publish the combined two batches of\n",
    "labeled raw data as v2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download text_raw_v2\n",
    "cp -rf dataset/text_cls_v2 data/text_raw_v2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Publish raw data v2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:dataci.dataset.publish:Caching dataset files: data/text_raw_v2/train.csv\n",
      "\u001b[?25l                                                                          \u001b[32m⠋\u001b[0m Checking graph\n",
      "Adding...                                                                       \n",
      "!\u001b[A\n",
      "  0% Checking cache in '/root/workspace/DataCI/.dvc/cache'| |0/? [00:00<?,    ?f\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0%|          |Transferring                          0/? [00:00<?,     ?file/s]\u001b[A\n",
      "  0%|          |Transferring                          0/1 [00:00<?,     ?file/s]\u001b[A\n",
      "100% Adding...|████████████████████████████████████████|1/1 [00:00, 43.10file/s]\u001b[A\n",
      "\u001b[0mINFO:dataci.dataset.publish:Adding dataset to db: text_raw_train@09d026c8a2f88dedff2f0c495b42975707e432e1\n"
     ]
    }
   ],
   "source": [
    "!python dataci/command/dataset.py publish -n text_raw_train data/text_raw_v2/train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can easily update our text classification dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:dataci.dataset.update:Searching changes...\n",
      "INFO:dataci.dataset.update:Found 2 possible updates:\n",
      "INFO:dataci.dataset.update:| S.N. | Parent dataset                 >>> Pipeline                       |\n",
      "INFO:dataci.dataset.update:|    1 | text_raw_train@09d026c         >>> train_data_pipeline@bcca51d    |\n",
      "INFO:dataci.dataset.update:|    2 | text_raw_train@09d026c         >>> train_data_pipeline@ceb5abe    |\n",
      "INFO:dataci.dataset.update:Total 1 dataset version(s), 2 pipeline versions(s).\n",
      "INFO:dataci.dataset.update:Executing dataset update...\n",
      "Stage 'text_augmentation' is cached - skipping run, checking out outputsre\u001b[39m>\n",
      "Generating lock file 'dvc.lock'                                                 \n",
      "Updating lock file 'dvc.lock'\n",
      "Use `dvc push` to send your updates to remote storage.\n",
      "\u001b[0mINFO:dataci.run.save:Recover dvc file feat/text_aug.csv.dvc\n",
      "INFO:dataci.dataset.publish:Caching dataset files: feat/text_aug.csv\n",
      "\u001b[?25l\u001b[32m⠋\u001b[0m Checking graph                                                 \n",
      "\u001b[1A\u001b[2K\u001b[31mERROR\u001b[39m: output 'feat/text_aug.csv' is already specified in stage: 'text_augmentation'.\n",
      "\u001b[0mINFO:dataci.dataset.publish:Adding dataset to db: train_data_pipeline:text_aug@b312fde6d3c1f55e98a282f616b485a8e2ae60c6\n",
      "INFO:dataci.pipeline.pipeline:text_raw_train@09d026c8a2f88dedff2f0c495b42975707e432e1 >>> train_data_pipeline@bcca51d.run2 >>> train_data_pipeline:text_aug@None\n",
      "INFO:dataci.dataset.update:Finish 1/2\n",
      "Stage 'text_augmentation' is cached - skipping run, checking out outputsre\u001b[39m>\n",
      "Generating lock file 'dvc.lock'                                                 \n",
      "Updating lock file 'dvc.lock'\n",
      "Use `dvc push` to send your updates to remote storage.\n",
      "\u001b[0mINFO:dataci.run.save:Recover dvc file feat/text_aug.csv.dvc\n",
      "INFO:dataci.dataset.publish:Caching dataset files: feat/text_aug.csv\n",
      "\u001b[?25l                                                                          \u001b[32m⠋\u001b[0m Checking graph\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR\u001b[39m: output 'feat/text_aug.csv' is already specified in stage: 'text_augmentation'.\n",
      "\u001b[0mINFO:dataci.dataset.publish:Adding dataset to db: train_data_pipeline:text_aug@d3c0a2f54c521d13fe4170ea6a812ffbdb8d4cca\n",
      "INFO:dataci.pipeline.pipeline:text_raw_train@09d026c8a2f88dedff2f0c495b42975707e432e1 >>> train_data_pipeline@ceb5abe.run2 >>> train_data_pipeline:text_aug@None\n",
      "INFO:dataci.dataset.update:Finish 2/2\n"
     ]
    }
   ],
   "source": [
    "!python dataci/command/dataset.py update -n train_data_pipeline:text_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Summary\n",
    "\n",
    "That is a long journey! Wait, how many dataset we have and what are their performance?\n",
    "It seems quite messy after we publish many datasets and pipelines, run a lot of workflows.\n",
    "Luckily, when we're developing our data pipelines, DataCI helps in managing and auditing all of them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.1 How many datasets are built??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_raw_train\n",
      "|  Version\tYield pipeline\tParent dataset\tSize\tCreate time\n",
      "|- f3a821d\tN.A.\t\tNone@None\t\t17057\t2023-03-24 13:50:57\n",
      "|- 09d026c\tN.A.\t\tNone@None\t\t35813\t2023-03-24 13:51:21\n",
      "text_raw_val\n",
      "|  Version\tYield pipeline\tParent dataset\tSize\tCreate time\n",
      "|- 641a430\tN.A.\t\tNone@None\t\t1896\t2023-03-24 13:50:58\n",
      "train_data_pipeline:text_aug\n",
      "|  Version\tYield pipeline\tParent dataset\tSize\tCreate time\n",
      "|- db07fae\ttrain_data_pipeline@bcca51d\t\ttext_raw_train@f3a821d8a22533c642f16f115d5d2b3f02569f0d\t\t17057\t2023-03-24 13:51:15\n",
      "|- c5a9a49\ttrain_data_pipeline@ceb5abe\t\ttext_raw_train@f3a821d8a22533c642f16f115d5d2b3f02569f0d\t\t17057\t2023-03-24 13:51:20\n",
      "|- b312fde\ttrain_data_pipeline@bcca51d\t\ttext_raw_train@09d026c8a2f88dedff2f0c495b42975707e432e1\t\t17057\t2023-03-24 13:51:23\n",
      "|- d3c0a2f\ttrain_data_pipeline@ceb5abe\t\ttext_raw_train@09d026c8a2f88dedff2f0c495b42975707e432e1\t\t17057\t2023-03-24 13:51:24\n"
     ]
    }
   ],
   "source": [
    "!python dataci/command/dataset.py ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.2 Compair between different dataset versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.3 How many pipelines are built?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_pipeline\n",
      "|  Version\tCreate time\n",
      "|- bcca51d\t2023-03-24 13:51:00\n",
      "|    |- run1\n",
      "|    |- run2\n",
      "|- ceb5abe\t2023-03-24 13:51:15\n",
      "|    |- run1\n",
      "|    |- run2\n"
     ]
    }
   ],
   "source": [
    "!python dataci/command/pipeline.py ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
