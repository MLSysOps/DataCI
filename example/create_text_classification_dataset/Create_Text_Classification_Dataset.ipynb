{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "In this tutorial, we are going to build a text dataset for category classification. After this tutorial,\n",
    "we will have a basic picture of how to use DataCI to manage different versions of datasets,\n",
    "their data generating pipelines, and quickly adapt previous data scientists efforts to new versions of datasets.\n",
    "\n",
    "This tutorial uses a simplified workflow from industry. Given a product title, we are going to determine the product\n",
    "category.\n",
    "\n",
    "# 0. Prerequisites\n",
    "\n",
    "## Initialize DataCI"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Code\\PycharmProjects\\DataCI\n"
     ]
    }
   ],
   "source": [
    "%cd ../../"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized DVC repository.\n",
      "\n",
      "+---------------------------------------------------------------------+\n",
      "|                                                                     |\n",
      "|        DVC has enabled anonymous aggregate usage analytics.         |\n",
      "|     Read the analytics documentation (and how to opt-out) here:     |\n",
      "|             <https://dvc.org/doc/user-guide/analytics>              |\n",
      "|                                                                     |\n",
      "+---------------------------------------------------------------------+\n",
      "\n",
      "What's next?\n",
      "------------\n",
      "- Check out the documentation: <https://dvc.org/doc>\n",
      "- Get help and share ideas: <https://dvc.org/chat>\n",
      "- Star us on GitHub: <https://github.com/iterative/dvc>\n"
     ]
    }
   ],
   "source": [
    "!python dataci/command/init.py -f"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Download Sample Raw Data\n",
    "\n",
    "Assume we have sampled 20K raw data from online product database, and hand over these raw data to annotators for\n",
    "verify their product category which are filled by sellers and contains some noise. Now, the first batch of\n",
    "10K finish labelling data are returned."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "# saved at data/text_raw/\n",
    "mkdir -p data\n",
    "rm -r data/*\n",
    "cp -r dataset/text_cls_v1 data/text_raw/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "This dataset contains train and val splits. Each split contains a CSV file with 3 columns:\n",
    "`id`, `product_name` and `category_lv3`. We are going to build a pipeline to classify the product category\n",
    "(`category_lv3`) based on its dirty `product_name`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Build Text Classification Dataset\n",
    "\n",
    "## 1.1 Publish raw data\n",
    "\n",
    "Add this dataset with two split into the data repository."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dataci.dataset.publish:Caching dataset files: data\\text_raw\\train.csv\n",
      "\\u280b Collecting stages from the workspace\n",
      "\\u2819 Checking graph\n",
      "\\u2839 Checking graph\n",
      "\\u2838 Checking graph\n",
      "\\u2838 Checking graph\n",
      "\n",
      "INFO:dataci.dataset.publish:Adding dataset to db: text_raw_train@63b4a736f55762612a9bd1c9d553e21eedc2d1db\n",
      "INFO:dataci.dataset.publish:Caching dataset files: data\\text_raw\\val.csv\n",
      "\\u280b Collecting stages from the workspace\n",
      "\\u2819 Checking graph\n",
      "\\u2839 Checking graph\n",
      "\\u2839 Checking graph\n",
      "\n",
      "INFO:dataci.dataset.publish:Adding dataset to db: text_raw_val@5a1615201e7e7542ff33647efa87ffc0e415936c\n"
     ]
    }
   ],
   "source": [
    "!python dataci/command/dataset.py publish -n text_raw_train data/text_raw/train.csv\n",
    "!python dataci/command/dataset.py publish -n text_raw_val data/text_raw/val.csv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Build a dataset for text classification\n",
    "\n",
    "1. Build train dataset v1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import augly.text as txtaugs\n",
    "\n",
    "from dataci.pipeline import Pipeline, stage\n",
    "\n",
    "\n",
    "# Data processing: text augmentation\n",
    "@stage(inputs='text_raw_train', outputs='text_aug.csv')\n",
    "def text_augmentation(inputs):\n",
    "    transform = txtaugs.InsertPunctuationChars(\n",
    "        granularity=\"all\",\n",
    "        cadence=5.0,\n",
    "        vary_chars=True,\n",
    "    )\n",
    "    inputs['product_name'] = inputs['product_name'].map(transform)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Define data pipeline\n",
    "train_data_pipeline = Pipeline(name='train_data_pipeline', stages=[text_augmentation])\n",
    "train_data_pipeline.build()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the train data pipeline:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<dataci.run.run.Run at 0x1fe7f756f70>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pipeline()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output `text_aug.csv` will be used as train dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Run training with the built train and val dataset v1\n",
    "Now you can simple train a pre-trained BERT on this text classification dataset v1:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python example/create_text_classification_dataset/train.py \\\n",
    "  --train_dataset=train_data_pipeline/latest/runs/1/feat/text_aug.csv \\\n",
    "  --test_dataset=../data/text_raw/val.csv \\\n",
    "  -b4 \\\n",
    "  --max_train_steps_per_epoch=20 \\\n",
    "  --max_val_steps_per_epoch=20"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For demonstration purpose, we only train and validation the dataset for a few steps and obtain the results.\n",
    "\n",
    "3. Save data pipeline\n",
    "\n",
    "You can now publish your data pipeline for a better management."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_data_pipeline.publish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Publish first version of text dataset\n",
    "\n",
    "Run the published pipeline `train_data_pipeline`, its final output `text_aug.csv` will be\n",
    "automatically published as a dataset: `train_data_pipeline:text_aug`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dataci.run.save:Recover dvc file feat\\text_aug.csv.dvc\n",
      "INFO:dataci.dataset.publish:Caching dataset files: feat\\text_aug.csv\n",
      "INFO:dataci.dataset.publish:Adding dataset to db: train_data_pipeline:text_aug@1c08c58212cbc0f87f93cf5e8a8a21f28f5547f4\n",
      "INFO:dataci.pipeline.pipeline:text_raw_train@63b4a736f55762612a9bd1c9d553e21eedc2d1db >>> train_data_pipeline@50addf3.run1 >>> train_data_pipeline:text_aug@None\n"
     ]
    },
    {
     "data": {
      "text/plain": "<dataci.run.run.Run at 0x1fe7f736970>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pipeline()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Try with New Data Augmentation Method\n",
    "\n",
    "Let's create a second version of `train_data_pipeline:text_aug` for text classification with\n",
    "different data augmentation method to improve the model performance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Write a second version train data pipeline\n",
    "\n",
    "We design a better data augmentation method for `train_data_pipeline_v2`:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "@stage(inputs='text_raw_train', outputs='text_aug.csv')\n",
    "def text_augmentation(inputs):\n",
    "    transform = txtaugs.Compose(\n",
    "        [\n",
    "            txtaugs.InsertWhitespaceChars(p=0.5),\n",
    "            txtaugs.InsertPunctuationChars(\n",
    "                granularity=\"all\",\n",
    "                cadence=5.0,\n",
    "                vary_chars=True,\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    inputs['product_name'] = inputs['product_name'].map(transform)\n",
    "    return inputs\n",
    "\n",
    "train_data_pipeline_v2 = Pipeline(name='train_data_pipeline', stages=[text_augmentation])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Test the pipeline v2 and publish"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_data_pipeline_v2()\n",
    "train_data_pipeline_v2.publish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Let's check our pipeline `train_data_pipeline`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_pipeline\n",
      "|  Version\tCreate time\n",
      "|- 50addf3\t2023-03-21 23:07:43\n",
      "|    |- run1\n",
      "|- fce2553\t2023-03-21 23:08:20\n"
     ]
    }
   ],
   "source": [
    "!python dataci/command/pipeline.py ls"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Publish text classification dataset v2\n",
    "\n",
    "It is easy to update output dataset once our data pipeline have new version:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dataci.run.save:Recover dvc file feat\\text_aug.csv.dvc\n",
      "INFO:dataci.dataset.publish:Caching dataset files: feat\\text_aug.csv\n",
      "INFO:dataci.dataset.publish:Adding dataset to db: train_data_pipeline:text_aug@9087a73a1540193c5f1172f9491bbd9743cb8401\n",
      "INFO:dataci.pipeline.pipeline:text_raw_train@63b4a736f55762612a9bd1c9d553e21eedc2d1db >>> train_data_pipeline@fce2553.run1 >>> train_data_pipeline:text_aug@None\n"
     ]
    },
    {
     "data": {
      "text/plain": "<dataci.run.run.Run at 0x1fe7f7369a0>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pipeline_v2()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Try with more raw data\n",
    "\n",
    "Our human annotators have finished the 2nd batch 10K data labelling. We publish the combined two batches of\n",
    "labeled raw data as v2:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download text_raw_v2\n",
    "cp -rf dataset/text_cls_v2 data/text_raw_v2/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Publish raw data v2:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dataci.dataset.publish:Caching dataset files: data\\text_raw_v2\\train.csv\n",
      "\\u280b Collecting stages from the workspace\n",
      "\\u2819 Checking graph\n",
      "\\u2839 Checking graph\n",
      "\\u2838 Checking graph\n",
      "\n",
      "INFO:dataci.dataset.publish:Adding dataset to db: text_raw_train@03f7a71b52b0441eac6675c408b120358bdd9519\n"
     ]
    }
   ],
   "source": [
    "!python dataci/command/dataset.py publish -n text_raw_train data/text_raw_v2/train.csv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can easily update our text classification dataset:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 'text_augmentation' is cached - skipping run, checking out outputs\n",
      "Generating lock file 'dvc.lock'\n",
      "Updating lock file 'dvc.lock'\n",
      "Use `dvc push` to send your updates to remote storage.\n",
      "Stage 'text_augmentation' is cached - skipping run, checking out outputs\n",
      "Generating lock file 'dvc.lock'\n",
      "Updating lock file 'dvc.lock'\n",
      "Use `dvc push` to send your updates to remote storage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dataci.dataset.update:Searching changes...\n",
      "INFO:dataci.dataset.update:Found 2 possible updates:\n",
      "INFO:dataci.dataset.update:| S.N. | Parent dataset                 >>> Pipeline                       |\n",
      "INFO:dataci.dataset.update:|    1 | text_raw_train@03f7a71         >>> train_data_pipeline@50addf3    |\n",
      "INFO:dataci.dataset.update:|    2 | text_raw_train@03f7a71         >>> train_data_pipeline@fce2553    |\n",
      "INFO:dataci.dataset.update:Total 1 dataset version(s), 2 pipeline versions(s).\n",
      "INFO:dataci.dataset.update:Executing dataset update...\n",
      "INFO:dataci.run.save:Recover dvc file feat\\text_aug.csv.dvc\n",
      "INFO:dataci.dataset.publish:Caching dataset files: feat\\text_aug.csv\n",
      "\\u280b Collecting stages from the workspace\n",
      "\\u280b Checking graph\n",
      "\n",
      "\u001B[31mERROR\u001B[39m: output 'feat\\text_aug.csv' is already specified in stage: 'text_augmentation'.\n",
      "INFO:dataci.dataset.publish:Adding dataset to db: train_data_pipeline:text_aug@900fa953496c242f40d375ff8427180eeea9fa9e\n",
      "INFO:dataci.pipeline.pipeline:text_raw_train@03f7a71b52b0441eac6675c408b120358bdd9519 >>> train_data_pipeline@50addf3.run2 >>> train_data_pipeline:text_aug@None\n",
      "INFO:dataci.dataset.update:Finish 1/2\n",
      "INFO:dataci.run.save:Recover dvc file feat\\text_aug.csv.dvc\n",
      "INFO:dataci.dataset.publish:Caching dataset files: feat\\text_aug.csv\n",
      "\\u280b Collecting stages from the workspace\n",
      "\\u2819 Checking graph\n",
      "\n",
      "\u001B[31mERROR\u001B[39m: output 'feat\\text_aug.csv' is already specified in stage: 'text_augmentation'.\n",
      "INFO:dataci.dataset.publish:Adding dataset to db: train_data_pipeline:text_aug@52ee6c88d0e2fe0057468b9e6d45720473a2cbf4\n",
      "INFO:dataci.pipeline.pipeline:text_raw_train@03f7a71b52b0441eac6675c408b120358bdd9519 >>> train_data_pipeline@fce2553.run2 >>> train_data_pipeline:text_aug@None\n",
      "INFO:dataci.dataset.update:Finish 2/2\n"
     ]
    }
   ],
   "source": [
    "!python dataci/command/dataset.py update -n train_data_pipeline:text_aug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Summary\n",
    "\n",
    "That is a long journey! Wait, how many dataset we have and what are their performance?\n",
    "It seems quite messy after we publish many datasets and pipelines, run a lot of workflows.\n",
    "Luckily, when we're developing our data pipelines, DataCI helps in managing and auditing all of them!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 How many datasets are built??"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_raw_train\n",
      "|  Version\tYield pipeline\tParent dataset\tSize\tCreate time\n",
      "|- 63b4a73\tN.A.\t\tNone@None\t\tN.A.\t2023-03-21 23:07:35\n",
      "|- 03f7a71\tN.A.\t\tNone@None\t\tN.A.\t2023-03-21 23:08:52\n",
      "text_raw_val\n",
      "|  Version\tYield pipeline\tParent dataset\tSize\tCreate time\n",
      "|- 5a16152\tN.A.\t\tNone@None\t\tN.A.\t2023-03-21 23:07:37\n",
      "train_data_pipeline:text_aug\n",
      "|  Version\tYield pipeline\tParent dataset\tSize\tCreate time\n",
      "|- 1c08c58\ttrain_data_pipeline@50addf3\t\ttext_raw_train@63b4a736f55762612a9bd1c9d553e21eedc2d1db\t\tN.A.\t2023-03-21 23:08:04\n",
      "|- 9087a73\ttrain_data_pipeline@fce2553\t\ttext_raw_train@63b4a736f55762612a9bd1c9d553e21eedc2d1db\t\tN.A.\t2023-03-21 23:08:46\n",
      "|- 900fa95\ttrain_data_pipeline@50addf3\t\ttext_raw_train@03f7a71b52b0441eac6675c408b120358bdd9519\t\tN.A.\t2023-03-21 23:09:22\n",
      "|- 52ee6c8\ttrain_data_pipeline@fce2553\t\ttext_raw_train@03f7a71b52b0441eac6675c408b120358bdd9519\t\tN.A.\t2023-03-21 23:09:27\n"
     ]
    }
   ],
   "source": [
    "!python dataci/command/dataset.py ls"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 Compair between different dataset versions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 How many pipelines are built?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_pipeline\n",
      "|  Version\tCreate time\n",
      "|- 50addf3\t2023-03-21 23:07:43\n",
      "|    |- run1\n",
      "|    |- run2\n",
      "|- fce2553\t2023-03-21 23:08:20\n",
      "|    |- run1\n",
      "|    |- run2\n"
     ]
    }
   ],
   "source": [
    "!python dataci/command/pipeline.py ls"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}